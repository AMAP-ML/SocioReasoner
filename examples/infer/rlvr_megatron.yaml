defaults:
  - ../config/deepspeed_zero@_here_
  - ../config/deepspeed_zero2@_here_
  - ../config/deepspeed_zero3@_here_
  - ../config/deepspeed_zero3_cpuoffload@_here_

hydra:
  run:
    dir: .
  output_subdir: null

exp_name: "qwen2_5_vl_3B_socioseg_infer"
seed: 42
logging_dir: ./output/infer/logs
output_dir: ./output/infer

pretrain: vvangfaye/SocioReasoner-3B
# pretrain: path/to/pretrain

checkpoint_config:
  type: file_system
  output_dir: ./output/infer/checkpoint
    
track_with: tensorboard
tracker_kwargs:
  log_dir: ./output/infer/tensorboard

save_steps: 20
logging_steps: 1
eval_steps: 20
resume_from_checkpoint: false

rollout_batch_size: 250
num_return_sequences_in_group: 1
is_num_return_sequences_expand: true
prompt_length: 4096
response_length: 2048
generate_opt_level: 0

ppo_epochs: 1
value_clip: 0.5
reward_clip: 10
advantage_clip: 10.0
# dual_clip_loss: true
# reward_shift: true
whiten_advantages: false
init_kl_coef: 0.0
adv_estimator: "grpo"
use_kl_loss: true
kl_loss_coef: 5.0e-3

actor_train:
  model_args:
    disable_gradient_checkpointing: false
    dtype: bf16
    model_type: ~
    max_pixels: 1344 * 1344
    min_pixels: 500 * 500
  training_args:
    learning_rate: 1.0e-6
    weight_decay: 1.0e-2
    per_device_train_batch_size: 2
    gradient_accumulation_steps: 8
    warmup_steps: 0
    num_train_epochs: 10
  data_args:
    template: qwen2-vl
    file_name: path/to/dataset/
    dataset_dir: 
    preprocessing_num_workers: 16
  strategy_args:
    strategy_name: megatron_train
    strategy_config:
      sequence_parallel: true
      tensor_model_parallel_size: 2
      context_parallel_size: 1
      expert_model_parallel_size: 1
      pipeline_model_parallel_size: 1
      overlap_grad_reduce: true
      use_distributed_optimizer: true
      bf16: true
  device_mapping: list(range(0,4))
  infer_batch_size: 8

actor_infer:
  model_args:
    disable_gradient_checkpointing: true
    dtype: bf16
  generating_args:
    max_new_tokens: ${response_length}
    top_p: 0.8
    top_k: 100
    num_beams: 1
    temperature: 1
    num_return_sequences: ${num_return_sequences_in_group}
  data_args:
    template: qwen2-vl
    response: solution
  strategy_args:
    strategy_name: vllm
    strategy_config:
      gpu_memory_utilization: 0.9
      block_size: 16
      disable_mm_preprocessor_cache: true
  num_gpus_per_worker: 1
  device_mapping: list(range(0,4))
  infer_batch_size: 24

seg_infer:
  model_args:
    model_name_or_path: facebook/sam2-hiera-large
    disable_gradient_checkpointing: true
    dtype: bf16
  strategy_args:
    strategy_name: seg_infer
  num_gpus_per_worker: 1
  device_mapping: list(range(0,4))
  infer_batch_size: 32

reference:
  model_args:
    disable_gradient_checkpointing: true
    dtype: bf16
    model_type: ~
  data_args:
    template: qwen2-vl
    response: solution
  strategy_args:
    strategy_name: megatron_infer
    strategy_config:
      sequence_parallel: true
      tensor_model_parallel_size: 1
      context_parallel_size: 1
      pipeline_model_parallel_size: 1
      expert_model_parallel_size: 1
      bf16: true
  device_mapping: list(range(0,4))
  infer_batch_size: 8

rewards:
  # vl pipeline support MathRuleRewardWorker only, at present.
  # Support for rewards in other domains will be retained for future implementation.
  math_rule:
    worker_cls: roll.pipeline.rlvr.rewards.socioseg_rule_reward_worker.SocioSegRuleRewardWorker
    model_args:
      model_name_or_path: ${pretrain}
    data_args:
      template: qwen2-vl
      response: solution
    strategy_args:
      strategy_name: hf_infer
      strategy_config: ~
    world_size: 16
    infer_batch_size: 4
